# -*- coding: utf-8 -*-
"""deepfake_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cELnA0EkVFH7R8hqd0SIwd2bY-9762VV

**DeepFake Detection - Final Year Project**
"""

#Unzip the dataset from zip file
#!unzip "/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake.zip" -d "/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/"

#Import TensorFlow and TensorFlow Hub into colab
import tensorflow as tf
import tensorflow_hub as hub
print("TensorFlow",tf.__version__)
print("TensorFlow",hub.__version__)

# Check for GPU availability
print("GPU","available (YES)" if tf.config.list_physical_devices("GPU") else "Not available")

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

# DeepFake Dataset
import os


def create_csv_file(image_folder, label, csv_file):
    # Get the list of image filenames in the folder
    image_files = os.listdir(image_folder)

    # Create a DataFrame to store image filenames and labels
    df = pd.DataFrame({'filename': image_files, 'label': label})

    # Save the DataFrame to a CSV file
    csv_file = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_dataset.csv'

    df.to_csv(csv_file, index=False)

if __name__ == "__main__":
    # Replace 'path_to_deepfake_folder' and 'path_to_real_folder' with the actual paths to your image folders
    deepfake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df'
    # real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/real'

    # Replace 'deepfake' and 'real' with the respective labels for each type of image
    deepfake_label = 'deepfake'
    # real_label = 'real'

    # Replace 'output_csv_file.csv' with the desired name of your CSV file
    # csv_file = 'output_csv_file.csv'

    # Create CSV file for deepfake images
    create_csv_file(deepfake_folder, deepfake_label, csv_file)

    # Create CSV file for real images
    # create_csv_file(real_folder, real_label, csv_file)

    print("CSV file created successfully.")

# Real Dataset
import os
import pandas as pd

def create_csv_file(image_folder, label, csv_file):
    # Get the list of image filenames in the folder
    image_files = os.listdir(image_folder)

    # Create a DataFrame to store image filenames and labels
    df = pd.DataFrame({'filename': image_files, 'label': label})

    # Save the DataFrame to a CSV file
    csv_file = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/real_dataset.csv'

    df.to_csv(csv_file, index=False)

if __name__ == "__main__":
    # Replace 'path_to_deepfake_folder' and 'path_to_real_folder' with the actual paths to your image folders
    # deepfake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df'
    real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/real'

    # Replace 'deepfake' and 'real' with the respective labels for each type of image
    # deepfake_label = 'deepfake'
    real_label = 'real'

    # Replace 'output_csv_file.csv' with the desired name of your CSV file
    # csv_file = 'output_csv_file.csv'

    # Create CSV file for deepfake images
    # create_csv_file(deepfake_folder, deepfake_label, csv_file)

    # Create CSV file for real images
    create_csv_file(real_folder, real_label, csv_file)

    print("CSV file created successfully.")

import pandas as pd
deepfake_data =pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_dataset.csv")
real_data=pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/real_dataset.csv")
deepfake_data.head()

deepfake_data.tail()

deepfake_data.info()

deepfake_data.describe()

real_data.head()

real_data.tail()

real_data.describe()

import os
import pandas as pd

def create_dataframe(image_folder, label):
    # Get the list of image file paths in the folder
    image_files = os.listdir(image_folder)
    image_paths = [os.path.join(image_folder, file) for file in image_files]

    # Create a list of image names by extracting the filename without the directory path and extension
    image_names = [os.path.splitext(os.path.basename(path))[0] for path in image_paths]

    # Create a list of corresponding labels
    labels = [label] * len(image_paths)

    # Create a DataFrame with 'filename', 'image_name', and 'label' columns
    df = pd.DataFrame({'filepath': image_paths, 'image_name': image_names, 'label': labels})

    return df

if __name__ == "__main__":
    # Replace 'path_to_real_folder' and 'path_to_fake_folder' with the actual paths to your image folders
    real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/real'
    fake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df'

    # Replace 'real' and 'fake' with the respective labels for each type of image
    real_label = 'real'
    fake_label = 'deepfake'

    # Create DataFrames for real and fake images
    real_df = create_dataframe(real_folder, real_label)
    fake_df = create_dataframe(fake_folder, fake_label)

    # Concatenate the two DataFrames to create the final combined dataset
    combined_df = pd.concat([real_df, fake_df], ignore_index=True)

    # Optionally, shuffle the dataset
    combined_df = combined_df.sample(frac=1).reset_index(drop=True)

    # Display the first few rows of the combined DataFrame
    print(combined_df.head())

    # Optionally, save the DataFrame to a new CSV file
    combined_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/combined_dataset.csv', index=False)

com=pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/combined_dataset.csv")
com.head()

com.tail()

com.describe()

com.info()

import os
import pandas as pd

def create_dataframe(image_folder, label):
    # Get the list of image file paths in the folder
    image_files = os.listdir(image_folder)
    image_paths = [os.path.join(image_folder, file) for file in image_files]

    # Create a list of image names by extracting the filename without the directory path and extension
    image_names = [os.path.splitext(os.path.basename(path))[0] for path in image_paths]

    # Create a list of corresponding labels
    labels = [label] * len(image_paths)

    # Create a DataFrame with 'filename', 'image_name', 'label', and 'file_path' columns
    df = pd.DataFrame({'filename': image_files, 'image_name': image_names, 'label': labels, 'file_path': image_paths})

    return df

if __name__ == "__main__":
    # Replace 'path_to_real_folder' and 'path_to_deepfake_folder' with the actual paths to your image folders
    real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/real'
    deepfake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df'

    # Replace 'real' and 'deepfake' with the respective labels for each type of image
    real_label = 'real'
    deepfake_label = 'deepfake'

    # Create DataFrames for real and deepfake images
    real_df = create_dataframe(real_folder, real_label)
    deepfake_df = create_dataframe(deepfake_folder, deepfake_label)
    combined_df.to_csv('real_df.csv', index=False)


    # Concatenate the two DataFrames to create the final combined dataset
    combined_df = pd.concat([real_df, deepfake_df], ignore_index=True)

    # Optionally, shuffle the dataset
    combined_df = combined_df.sample(frac=1).reset_index(drop=True)

    # Display the first few rows of the combined DataFrame
    print(combined_df.head())

    # Optionally, save the DataFrame to a new CSV file
    combined_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/both_dataset.csv', index=False)

both =pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/both_dataset.csv')
both.head()

both.tail()

both.describe()

both.info()

both['label'].value_counts()

both['label'].value_counts().plot.bar(figsize=(5,5))

from IPython.display import Image
Image("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df/df04391.jpg")

import os
import pandas as pd

def create_dataframe(image_folder, label):
    # Get the list of image file paths in the folder
    image_files = os.listdir(image_folder)
    image_paths = [os.path.join(image_folder, file) for file in image_files]

    # Create a list of image names by extracting the filename without the directory path and extension
    image_names = [os.path.splitext(os.path.basename(path))[0] for path in image_paths]

    # Create a list of corresponding labels
    labels = [label] * len(image_paths)

    # Create a DataFrame with 'filename', 'image_name', 'label', 'binary_label', and 'file_path' columns
    df = pd.DataFrame({'filename': image_files, 'image_name': image_names, 'label': labels, 'file_path': image_paths})

    return df

if __name__ == "__main__":
    # Replace 'path_to_real_folder' and 'path_to_deepfake_folder' with the actual paths to your image folders
    real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/real'
    deepfake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/train/df'

    # Create DataFrames for real and deepfake images
    real_df = create_dataframe(real_folder, 'real')
    deepfake_df = create_dataframe(deepfake_folder, 'deepfake')

    # Combine the DataFrames for real and deepfake images
    combined_df = pd.concat([real_df, deepfake_df], ignore_index=True)

    # Create a binary label column where 'real' is labeled as 0 and 'deepfake' is labeled as 1
    combined_df['binary_label'] = combined_df['label'].apply(lambda x: 0 if x == 'real' else 1)

    # Optionally, shuffle the dataset
    combined_df = combined_df.sample(frac=1).reset_index(drop=True)

    # Display the first few rows of the combined DataFrame
    print(combined_df.head())

    # Save the combined DataFrame to a CSV file
    combined_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/both_Label_dataset.csv', index=False)

both_lab=pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/both_Label_dataset.csv")
both_lab.head()

both_lab.tail()

import numpy as np
labels=both_lab['label']
labels=np.array(labels)
labels

len(labels)

unique_labels=np.unique(labels)
unique_labels

len(unique_labels)

print(labels[0])
labels[0]==unique_labels

boolean_labels=[label==unique_labels for label in labels]
boolean_labels

len(boolean_labels)

print(labels[0])
print(np.where(unique_labels == labels[0]))
print(boolean_labels[0].astype(int))
print(boolean_labels[2].astype(int))
boolean_labels= np.array(boolean_labels).astype('int')
boolean_labels

# Creating Validation set fot the dataset
# setup x and y variable
x=both_lab['file_path'].values
y=boolean_labels
x,len(x),y,len(y)

#Setup number of images for experimentation
num_images =1000 #@param {type:'slider', min:1000,max:10000,step:100}

#Split the data for training
from sklearn.model_selection import train_test_split

#split the data into training and validation of total images
x_train,x_val,y_train,y_val =train_test_split(x[:num_images],
                                              y[:num_images],
                                              test_size=0.2,
                                              random_state=45)
len(x_train),len(y_train),len(x_val),len(y_val)

x_train[:3],y_train[:3]

"""**PreProcessing the Images and Turning them into Tensors**"""

# First Convert Images to Numpy array
from matplotlib.pyplot import imread
image=imread(x[8])
image

image.shape

# Creating a function to convert image to tensor

img_size =224

def process_image(image_path):
  # Take the image path and turn it into tensors
  # Read the image file using image path
  image =tf.io.read_file(image_path)
  # turn jpg image into numerical tensors with 3 color channels
  image=tf.image.decode_jpeg(image,channels=3)
  # convert color channel values from 1-255 to 0-1 values
  image =tf.image.convert_image_dtype(image,tf.float32)
  # resize the image to our desired value(244,244)
  image=tf.image.resize(image,size=[img_size,img_size])

  return image

"""**Divide the images into batches of 32 image so it could fit into the computers memory and process image faster**"""

# create a functin to return a tuple(image,label)
def get_image_label(image_path,label):
  image =process_image(image_path)
  return image,label

x[45],y[45]

# Demo for above
(process_image(x[45]),tf.constant(y[45]))

# Turn all our data into batches of size 32 for both x and y
batch_size=32

# Now create a function to turn data into batches
def create_data_batches(x,y=None,batch_size=batch_size,valid_data=False,test_data=False):
  # if data is test dataset, we don't have labels
  if test_data:
    print("Creating test data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(x))) #only file path (no labels)
    data_batch =data.map(process_image).batch(batch_size)
    return data_batch

  # if the data is valid dataset ,we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y))) #filepath and labels
    data_batch =data.map(get_image_label).batch(batch_size)
    return data_batch

  else:
    print("Creating training data batches...")
    # Turn filepaths and labels into tensors
    data =tf.data.Dataset.from_tensor_slices((tf.constant(x),tf.constant(y)))
    # Shuffling the pathnames and labels before mapping the image processor function is faster than shuffling images
    data =data.shuffle(buffer_size=len(x))
    # Create (image,label) tuple ,This also turns image path into preprocessed images
    data =data.map(get_image_label)
    # Turn the training data into batches
    data_batch =data.batch(batch_size)
    return data_batch

# Creating training and validation data batches using previously build function with parameters
train_data = create_data_batches(x_train,y_train)
val_data =create_data_batches(x_val,y_val,valid_data=True)

"""**Visualizing the data batches**"""

import matplotlib.pyplot as plt

# Creating a function for viewing image in data batches

def show_25_images(images,labels):
  plt.figure(figsize=(15,15))
  # Loop throught 25 images
  for i in range(25):
    ax=plt.subplot(5,5,i+1)
    # Display images
    plt.imshow(images[i])
    # Add image label as title
    plt.title(unique_labels[labels[i].argmax()])
    # Turn the grid line off
    plt.axis("off")

train_data

val_data

# Visualize on Training data
train_images,train_labels =next(train_data.as_numpy_iterator())
show_25_images(train_images,train_labels)

# Visualize on validatation data
val_images,val_labels=next(val_data.as_numpy_iterator())
show_25_images(val_images,val_labels)

"""**Building the Model**"""

# setup input shspe to our model same as that of trained model

input_shape=[None,img_size,img_size,3] #batch,height,width,color channel

# setup output shape to our model
output_shape=len(unique_labels)

#setup model url from tensorflow hub
model_url ="https://tfhub.dev/tensorflow/resnet_50/classification/1"

# create a function which builds a keras model
def create_model(input_shape=input_shape,output_shape=output_shape,model_url=model_url):
  print("Building model with:",model_url)

  # setup model layers
  model = tf.keras.Sequential([
      hub.KerasLayer(model_url), #layer 1 (input layer)
      tf.keras.layers.Dense(units=output_shape,
                            activation="softmax") # layer 2 (output layer)
  ])

  # compile the model

  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )

  # build the model
  model.build(input_shape)

  return model

model =create_model()
model.summary()

"""**Creating a callback function to save the model progress and stop training early if the model stops improving**"""

# Commented out IPython magic to ensure Python compatibility.
# Callback using TensorBoard

# Load TensorBoard notebook extention
# %load_ext tensorboard

import datetime
import os

# Creating a function to call tensorboard callback

def create_tensorboard_callback():
  # create log directory for storing tensorboard log data
  logdir = os.path.join('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/tensorboard_logs',
                        # Making it log datatime to keep track
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

# Early stopping callback ,stops our model from overfitting

early_stopping =tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                 patience=3)

"""**Training a model (on subset of data) our first model will train on 1000 images as experimental**"""

num_epochs=100 #@param {type:'slider',min:10,max:100,step:10}

# Check GPU availability
print("GPU available" if tf.config.list_physical_devices("GPU") else "Not Available")

"""**Creating the Model**"""

# Create a function that trains a model
def train_model():

  # create a model
  model=create_model()

  # Create new tensorboard session everytime we train model
  tensorboard = create_tensorboard_callback()

  # Fit the model
  model.fit(x=train_data,
            epochs=num_epochs,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard,early_stopping])

  # Return the fitted model
  return model

# Fit the model to data
model = train_model()

# Commented out IPython magic to ensure Python compatibility.
# Checking the tensorboard logs
import tensorboard
# %tensorboard --logdir="/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/tensorboard_logs"

val_data

# Make prediction on validation data(Not used to train)
predictions = model.predict(val_data)
predictions

predictions[0],len(predictions[0]),len(predictions)

predictions.shape

np.sum(predictions[0])

len(y_val)

len(unique_labels)

# Let's see the predictions
index=7
print(predictions[index])
print(f"Max value (probability of prediction - proba): {np.max(predictions[index])}")
print(f"Sum: {np.sum(predictions[index])}")
print(f"Predicted label: {unique_labels[np.argmax(predictions[index])]}")

"""**Predicting labels at large scale**"""

# Turn prediction probability into their respective label

def get_pred_label(prediction_probabilities):
  return unique_labels[np.argmax(prediction_probabilities)]

# get a predicted label based on array of prediction probabilities
pred_label =get_pred_label(predictions[81])
pred_label

val_data

"""**Creating a function to compare prediction data with validation data**

The val_data is in batch dataset and we have to unbatch it
"""

images_=[]
labels_=[]

# Loop through unbatched data
for image ,label in val_data.unbatch().as_numpy_iterator():
  images_.append(image)
  labels_.append(label)
  # print(image,label)
images_[1],labels_[1]

# We have unbatched data
len(images_),len(labels_)

images_[46],labels_[46]

get_pred_label(labels_[11])

get_pred_label(predictions[11])

len(val_data)

"""**Unbatch the batched data**"""

# Create a function to unbatch the batched dataset
def unbatchify(data):
  images=[]
  labels=[]

  # Loop through unbatched data
  for image,label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_labels[np.argmax(label)])
    return images,labels

#Unbatchifying the validation data
val_images,val_labels=unbatchify(val_data)
val_images[0],val_labels[0],len(val_images),len(val_labels)

get_pred_label(val_labels[0])

images_ = []
labels_ = []

# loop through unbatched data
for image ,label in val_data.unbatch().as_numpy_iterator():
  images_.append(image)
  # labels_.append(label)
  labels_.append(unique_labels[np.argmax(label)])
images_[1],labels_[1]
len(images_),len(labels_)

get_pred_label(labels_[9])

get_pred_label(predictions[0])

"""**Plotting the predictions**"""

def plot_pred(prediction_probabilities,labels,images,n=0):
  pred_prob,true_label,image=prediction_probabilities[n],labels[n],images[n]

  # get pred label
  pred_label =get_pred_label(pred_prob)

  # plot image and remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # change the color of title depending on prediction
  if pred_label == true_label:
    color="green"
  else:
    color="red"

  # Change plot title to be predicted, probability of prediction and truth label
  plt.title("{} {:2.0f}% {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
            color=color)
pred_label

predictions

plot_pred(prediction_probabilities=predictions,
          labels=labels_,
          images=images_,
          n=13)

"""**Visualizing the top predictions to find accuracy**"""

predictions[0]

predictions[0].argsort()

predictions[0][predictions[0].argmax()]

predictions[0].max()

unique_labels[predictions[0].argsort()]

# Create a function to visualize top 10 predicted label
def plot_pred_conf(prediction_probabilities,labels,n=1):
  pred_prob,true_label =prediction_probabilities[n],labels[n]

  # get predicted label
  pred_label = get_pred_label(pred_prob)

  # find top 10 predicted indexes
  top_10_pred_indexes =pred_prob.argsort()

  # find top 10 predicted indexes value
  top_10_pred_values =pred_prob[top_10_pred_indexes]

  # find top 10 predicted labels
  top_10_pred_labels = unique_labels[top_10_pred_indexes]

  # setup plot
  top_plot=plt.bar(np.arange(len(top_10_pred_labels)),
                   top_10_pred_values,
                   color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")

  # change color of true label
  if np.isin(true_label , top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=labels_,
               n=98)

# lets check few more predictions and their different values
i_multiplier =20
num_rows =3
num_cols =2
num_images =num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows,2*num_cols ,2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=labels_,
            images=images_,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=labels_,
                 n=i+i_multiplier)

plt.tight_layout(h_pad=1.0)
plt.show()

"""**Saving our model with functiona**"""

#Create a function to save the model
# save to model to given directory with suffix filename
def save_model(model,suffix=None):
   # create a model with directory pathname with current time
  modeldir=os.path.join("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/models",
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
    # save the model in formet
  model_path=modeldir + "-" + suffix + ".h5"
  print(f"Saving the model to :{model_path}")
  model.save(model_path)

  return model_path

"""**Loading our model with function**"""

# Create a function to load our model
def load_model(model_path):
  # load the model from specific path
  print(f"Loading the saved model from :{model_path}")
  model=tf.keras.models.load_model(model_path,custom_objects={"KerasLayer":hub.KerasLayer})

  return model

"""**Save our model with less trained data**"""

#save our model trained on 1000 images
save_model(model,suffix="1000-images-resnet50-Adam")

# Load our less trained model
load_1000_images_model=load_model("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/models/20230819-08251692433542-1000-images-resnet50-Adam.h5")

# Evaluate the pre saved model
model.evaluate(val_data)

#evaluate the loaded model
load_1000_images_model.evaluate(val_data)

"""**Training our Model with the entire dataset (1000+ images)**"""

len(x),len(y)

len(x_train),len(y_train)

# Create a data batch with full dataset
full_data=create_data_batches(x,y)

full_data

# Create a model for full data
full_model=create_model()

# Create full model Callbacks
full_model_tensorboard=create_tensorboard_callback()

# No validation set when training on all the data,
#  so we can't monitor validation accuracy
full_model_early_stopping=tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                           patience=3)

"""**Running the full data will take larger time (more than 30 mins in runtime to run first epochs to load the data to memory)**"""

# Fit the full model to full data
full_model.fit(x=full_data,
               epochs=num_epochs,
               callbacks=[full_model_tensorboard,full_model_early_stopping])

save_model(full_model,suffix="full-image-set-resnet50-Adam-with-early-stop")

model_with_earlystop=load_model("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/models/20230820-06421692513779-full-image-set-resnet50-Adam-with-early-stop.h5")

model_with_earlystop.evaluate(val_data)

# Fit the full model to full data without callbacks
full_model.fit(x=full_data,
               epochs=num_epochs,
              #  callbacks=[full_model_tensorboard,full_model_early_stopping]
               )

save_model(full_model,suffix="full-image-set-resnet50-Adam-without-early-stop")

model_without_earlystop=load_model("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/models/20230820-07571692518239-full-image-set-resnet50-Adam-without-early-stop.h5")

model_without_earlystop.evaluate(val_data)

"""**Predicting the model with test data**"""

# Creating new dataset for test images

import os
import pandas as pd

def create_dataframe(image_folder, label):
    # Get the list of image file paths in the folder
    image_files = os.listdir(image_folder)
    image_paths = [os.path.join(image_folder, file) for file in image_files]

    # Create a list of image names by extracting the filename without the directory path and extension
    image_names = [os.path.splitext(os.path.basename(path))[0] for path in image_paths]

    # Create a list of corresponding labels
    labels = [label] * len(image_paths)

    # Create a DataFrame with 'filename', 'image_name', 'label', 'binary_label', and 'file_path' columns
    df = pd.DataFrame({'filename': image_files, 'image_name': image_names, 'label': labels, 'file_path': image_paths})

    return df

if __name__ == "__main__":
    # Replace 'path_to_real_folder' and 'path_to_deepfake_folder' with the actual paths to your image folders
    real_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/test/real'
    deepfake_folder = '/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/deepfake_database/test/df'

    # Create DataFrames for real and deepfake images
    real_df = create_dataframe(real_folder, 'real')
    deepfake_df = create_dataframe(deepfake_folder, 'deepfake')

    # Combine the DataFrames for real and deepfake images
    combined_df = pd.concat([real_df, deepfake_df], ignore_index=True)

    # Create a binary label column where 'real' is labeled as 0 and 'deepfake' is labeled as 1
    combined_df['binary_label'] = combined_df['label'].apply(lambda x: 0 if x == 'real' else 1)

    # Optionally, shuffle the dataset
    combined_df = combined_df.sample(frac=1).reset_index(drop=True)

    # Display the first few rows of the combined DataFrame
    print(combined_df.head())

    # Save the combined DataFrame to a CSV file
    combined_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/test_predictions/test_label_dataset.csv', index=False)

test_dataset=pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/test_predictions/test_label_dataset.csv")
test_dataset.head()

test_x =test_dataset['file_path']
test_x,len(test_x)

# Create test batches with test data
test_data =create_data_batches(test_x,test_data=True)
test_data

val_data

"""**Predicting with the test data**"""

test_data_predictions =model_with_earlystop.predict(test_data,
                                                    verbose=1)
test_data_predictions

val_data_predictions = model_with_earlystop.predict(val_data,
                                                    verbose=1)
val_data_predictions

"""**Save the Predictions to CSV file**"""

np.savetxt("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/test_predictions/test_pred_array.csv",
           test_data_predictions,
           delimiter=",")
np.savetxt("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/test_predictions/val_pred_array.csv",
           val_data_predictions,
           delimiter=",")

dataset=pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/test_predictions/test_pred_array.csv")
dataset.head()

dataset.info()

"""**Custom image prediction from user**"""

custom_path="/content/drive/MyDrive/Colab_Notebooks/Deepfake_Detection/custom_user_data/"
custom_image_path=[custom_path + fname for fname in os.listdir(custom_path)]
custom_image_path

os.listdir(custom_path)

#Create data batches for custom images
custom_data = create_data_batches(custom_image_path,
                                  test_data=True)
custom_data

# Predicting custom images with model
custom_preds = model_with_earlystop.predict(custom_data)
custom_preds

custom_preds.shape

# Get custom image prediction label
custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Unbatch the data to get the images to plot
custom_images=[]
# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

import matplotlib.pyplot as plt

from tensorflow.python.autograph.operators.py_builtins import enumerate_
# Plot and Check the custom image Predictions
plt.figure(figsize=(10,10))
for i , image in enumerate_(custom_images):
  plt.subplot(1,4,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)

"""**Working and predicted all the custom images correctly**"""